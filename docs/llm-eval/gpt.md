Methodologies for Evaluating Code Summary Quality and Model Understanding

Evaluating which model produces the best code summaries (for search and navigation) requires a combination of factual checks, semantic similarity measures, user-centric quality ratings, and search relevance tests. Below, we outline a comprehensive evaluation strategy with appropriate metrics, along with improvements to your current 5-phase pipeline.

Key Evaluation Objectives and Quality Dimensions

When comparing LLMs for code understanding, the evaluation should cover both factual accuracy and usefulness for navigation. In practice, this breaks down into several dimensions:
	•	Correctness & Accuracy: Does the summary correctly reflect the code’s actual behavior, signatures, and relationships (no hallucinations or false claims)?
	•	Completeness & Coverage: Does it mention all important aspects (function purpose, parameters, return values, dependencies, side effects) without omitting key details?
	•	Usefulness & Relevance: Is the summary helpful for a developer browsing the code (e.g. capturing the intent and context so it aids search queries)?
	•	Conciseness & Clarity: Is it easy to read and not overly verbose, while still conveying the needed information?
	•	Performance Considerations: Speed of generation and cost, which affect practical deployment (though these are separate from quality, they should be tracked in comparison).

These align well with common human evaluation criteria for code summaries like content adequacy (accuracy/completeness) and conciseness ￼. Your current metrics (Correctness, Completeness, Usefulness, Conciseness, Speed, Cost) cover many of these – below we suggest how to strengthen and extend them.

1. Automated AST-Based Factual Evaluation

Leverage your existing Tree-sitter AST parsing across languages to automatically validate factual content of summaries:
	•	AST Correctness Checks: Continue comparing each summary against ground-truth code facts extracted via AST (exports, class/function names, parameters, return types, async/await, imports, etc.). This catches blatant errors or hallucinations (e.g. mentioning a function that doesn’t exist, or wrong parameter) and ensures the summary’s claims match the code structure ￼. Automated fact-checking is critical since n-gram overlap metrics like BLEU/ROUGE notoriously fail to capture semantic correctness ￼ ￼. By using AST verification, you directly guard against hallucinations and factual inconsistencies (a known issue with LLM outputs ￼).
	•	Improved Completeness Scoring: Rather than just binary field presence, enhance the completeness metric to account for coverage of content. For example, ensure the summary not only mentions parameters/returns but gives at least a brief description of each. You could assign partial credit for each element of the ground truth covered. For instance, if the AST shows a function has 3 parameters and the summary only describes 2, it should score lower on completeness. This makes the completeness score more sensitive to how thoroughly a model documents the code’s elements (beyond the current ~87% plateau where all models just mention something for each field).
	•	Weighting by Importance: Consider weighting certain facts more heavily in correctness/completeness. For navigation, module-level relations (like high-level purpose and external dependencies) might be very important in file summaries, whereas parameter details are crucial in function summaries. If a summary misses a critical dependency or the main export of a file, that’s a bigger miss than omitting a minor implementation detail. Tuning the scoring to penalize critical omissions or mistakes more strongly will better differentiate model quality.
	•	Cross-File Consistency: Since code navigation is context-rich, you might analyze whether summaries maintain consistency about cross-file relationships. For example, if File A’s summary says it “invokes utility from File B,” ensure File B’s summary indeed describes providing that utility. While fully automating this is hard, you could use the AST dependency info: if file X imports module Y, check if X’s summary mentions the functionality from Y (and conversely Y’s summary mentions being used for X’s purpose). This consistency check can be an advanced completeness measure to see if the model captures how code components relate to each other, which is crucial for understanding larger codebases.

By bolstering AST-based checks, you ensure factual correctness (the foundation of any quality summary) is rigorously measured. This automated 50% of the score is already a strong point of your system – doubling down here will further reduce hallucinations and highlight models that truly “read” the code.

2. Semantic Similarity and Embedding-Based Metrics

In addition to structural accuracy, you should assess semantic understanding – i.e., how well the summary captures the meaning and intent of the code. Traditional text overlap metrics are insufficient for this (they have low correlation with human judgments on code summaries ￼). Instead, consider metrics that use embeddings or model-based semantics:
	•	Code–Summary Embedding Similarity: Represent the code and the summary in a vector space (for example, using a pretrained model like CodeBERT/GraphCodeBERT, or OpenAI’s code embedding model). Then compute the cosine similarity between the code’s embedding and its summary’s embedding. The intuition is that a high-quality summary will be semantically close to the code it describes. If the summary is off-topic or missing context, the embeddings will diverge. This can be done with specialized models that handle code+text or by embedding both into a common space. For instance, OpenAI demonstrates code search by embedding natural language queries and code in the same space, then using cosine similarity to find matches ￼. We can repurpose this idea: treat the summary like a natural language query/description of the code and measure how well it retrieves the actual code. A higher similarity indicates the summary captures the code’s semantics more effectively. This forms a reference-free semantic metric to complement AST checks.
	•	BERTScore or Sentence-BERT Metric: If you have reference documentation (e.g. existing code comments or README descriptions), you could use embedding-based similarity between the generated summary and reference text. Even without human-written references, you can use self-similarity: e.g., compare the model’s summary to a simple automatically generated baseline (like an AST pretty-printer that lists function names and types). A good model summary should significantly improve upon a trivial baseline in terms of semantic richness. Metrics like BERTScore (which uses contextual embeddings to compare similarity to a reference) have been used in code summarization; they correlate better with human opinion than BLEU, though still not perfectly ￼ ￼. If a trusted reference summary is available, BERTScore or METEOR etc. could be calculated – but since your use case likely involves undocumented code, reference-free metrics are more relevant.
	•	Key Concept Recall: Another automated check is whether the summary includes important domain terms or concepts present in the code. For example, if the code uses a specific algorithm or design pattern (say it implements a “binary search” or uses a “cache”), does the summary mention that? You can derive a list of salient terms from the code (identifiers, library names, etc.) and see if the summary incorporates or explains them. This can be done by simple keyword matching or more advanced concept extraction. It’s a proxy for how informative the summary is for search: a summary that includes high-level keywords (beyond just variable names) is more likely to be useful when searching by concept. While hard to fully automate for every concept, even checking for presence of class names, function names, or imported module names in the summary could be insightful (e.g., if code imports json and uses it, a good summary might say “parses JSON”).

Why embeddings? Embedding-based evaluation captures nuances that rigid AST checks or n-gram overlaps miss – for example, paraphrasing and semantic equivalence. Prior work suggests embedding metrics like Sentence-BERT have better (though still moderate) correlation with human judgments than BLEU/METEOR ￼. They are not perfect alone, but when combined with other methods they improve robustness. In fact, researchers recommend combining LLM-based judging with an embedding metric to mitigate bias (e.g., an LLM might favor its own style output) ￼. So, using an embedding similarity score alongside your LLM judge can help ensure no single evaluation method dominates unfairly.

3. LLM-as-Judge for Usefulness and Clarity

Your system already uses LLM-based judges – this is a cutting-edge approach and highly effective when done carefully. Recent studies confirm that prompting an LLM to evaluate summaries can achieve very high agreement with human evaluations ￼. To get the most out of it:
	•	Multi-dimensional Scoring Rubric: Ensure the LLM judge is asked to rate distinct aspects like usefulness for understanding, conciseness, and clarity/fluency. You mentioned currently capturing “usefulness” (perhaps akin to how well it helps a developer understand) and “conciseness”. You might also explicitly rate fluency or coherence – if a summary is factually correct but poorly worded or hard to read, that reduces its value. An LLM can score fluency easily. In research, LLM evaluators often consider coherence, consistency, fluency, and relevance as separate dimensions ￼. Your “usefulness” likely blends coherence/relevance, but it may help to break it down or at least define it clearly for the judge.
	•	Role-Playing Prompts: Improve the reliability of the LLM judge by using role prompts or chain-of-thought. For example, instruct the judge “You are a senior developer reviewing a code summary for accuracy and helpfulness…” before it scores. A 2024 method called CodeRPE had an LLM play roles like code author, reviewer, etc., to evaluate different aspects, achieving over 81% correlation with human ratings ￼. This suggests that more explicit, structured prompts can yield more consistent judgments. You might experiment with multi-step judging: first have the LLM list factual errors or missing info it notices (as a “code reviewer”), then separately have it rate usefulness (as a “new team member trying to understand code”), etc. Combining these perspectives could reduce random variance and bias in the scores.
	•	Consensus from Multiple Judges: You already use multiple judges (GPT-4, Claude, etc.) and take a median – continue this practice. It helps iron out individual LLM idiosyncrasies. If feasible, include a mix of model types (e.g. one from OpenAI, one from Anthropic) so that no single model’s bias dominates. This LLM ensemble judging is valuable for robustness.
	•	Bias Mitigation: Be aware of potential biases: for instance, an LLM judge might inadvertently favor summaries written in a style similar to its own. To combat this, use a different model as judge than the ones generating summaries (which it sounds like you are doing). Also, avoid revealing which model produced which summary (blind judging). Your BlindJudge component likely addresses this – ensuring the judge sees just the content, not any model identifier. These measures preserve fairness when comparing commercial APIs vs open-source models.
	•	Periodic Human Calibration: While not always possible at scale, consider having human experts review a subset of the summaries and the LLM scores. This can validate that the LLM judges are aligned with human preferences (and you can adjust prompts if not). Even an occasional manual review of outlier cases (where models’ scores differ widely or a summary scored high by LLM but looks questionable) can provide insights to refine the evaluation criteria.

Insight: The LLM-as-judge approach is considered one of the most reliable modern methods for evaluating nuanced quality ￼. It captures aspects like helpfulness and clarity that automated metrics miss. With careful prompt engineering and multiple judges, you can approach human-level evaluation fidelity ￼. Just remain mindful of consistency – e.g., use a fixed prompt template and perhaps a calibration set to ensure scores mean the same thing across time and models.

4. Information Retrieval & Search Relevance Testing

Because your end goal is to use these summaries for code search and navigation, it’s crucial to evaluate how well each model’s summaries serve that purpose. This is a more extrinsic evaluation: instead of just scoring the summary itself, you test its impact in a search scenario. Here are methodologies to do that:
	•	Embedding-based Retrieval Simulation: Build a simple retrieval experiment where each code snippet/file is represented by its summary (as if the summary is what you index in a search engine). Then, for a given query, see if the correct code is retrieved based on the summary text. You can simulate queries by using function names or docstrings as proxies for a search query, or by creating natural language questions about the code. For example, if one of the code files is a sorting function, a query might be “How to sort a list using merge sort?” and check if the summary for that function makes it rank highly. Use IR metrics like Recall@K or Mean Reciprocal Rank (MRR) to quantify success. If model A’s summaries consistently lead to the right code being found at rank 1 while model B’s don’t, then model A provides more search-effective summaries. This directly measures the navigation utility of the summaries.
	•	Code-Search Ground Truth: If you have access to a dataset of known code search queries and relevant documents (for instance, something like CodeSearchNet or an internal dataset of search queries and clicked results), you can use those to test each set of summaries. Index the summaries from each model and see how well the relevant code is retrieved. A summary that properly captures code semantics should improve search relevance even for queries that use higher-level language not verbatim in the code. For example, a user might search “user authentication logic” and a good summary of a file (auth.js) would mention “handles user authentication…” allowing it to be found, whereas a poor summary might just say “utility functions” and be missed. Measuring this requires some ground truth mapping of queries to code, which can be labor-intensive to create. If you don’t have real user queries, you could generate some synthetic ones: e.g., take the reference or your own understanding of the code’s purpose and frame natural questions.
	•	Vector Similarity Alignment: Another approach (related to the embedding metric above) is to ensure that for each code snippet, its own summary is the most similar to it (as opposed to summaries of other code). You can take all code embeddings and all summary embeddings and see if the pairing of each code with its summary has the highest cosine similarity among all pairings. A higher rate of correct code-summary pairing indicates that summaries are specific and semantically faithful (they “stick” to their source code). If a summary is so generic or off-base that it is closer to some other code snippet’s embedding, that’s a bad sign. This evaluation can produce a metric like “percentage of summaries that most closely match their own code embedding” – ideally 100%. Any deviation means the summary could be misleading for retrieval.
	•	Term Overlap for Search: Simpler but sometimes insightful: check how many code identifiers or important terms are present in the summary. For search, having some of the code’s keywords can be useful (e.g., class names, key method names). However, since the goal is also to abstract beyond code, a great summary might use synonyms or descriptions that help search. You could measure term overlap with a twist: count not only exact token overlap with code, but also synonyms or related terms. For instance, if the code variable is named calculateSHA256, a summary might not use that exact term but say “computes a SHA-256 hash” – which is actually better for search. This is hard to quantify automatically, but you might use an NLP technique or a domain lexicon to match “SHA256” with “SHA-256 hash” etc. Alternatively, use a language model to judge if the summary covers the “key topics” of the code (this becomes a meta-evaluator task).

Why test search directly? Ultimately, the value of these summaries will be measured by how much they assist developers in finding and understanding code. Including an IR-style evaluation ensures you don’t over-optimize for metrics like AST correctness while neglecting the practical usefulness of the summary in a real workflow. For example, a summary could score perfectly on AST correctness (no factual errors) but still be unhelpful (“This file defines a function.” – correct but not useful). By incorporating search relevance and usefulness metrics, you ensure the “best” model is truly the one that aids navigation.

5. Additional Metrics and Considerations

Beyond the core evaluations above, here are some additional metrics and improvements to consider for a thorough comparison:
	•	Consistency and Cohesion: A good summary should be coherent and logically structured. While this is partly captured by a clarity/fluency score from LLM judges, you could have a lightweight check for any obvious issues (like the summary contradicting itself or starting to describe one function then drifting). Ensuring each summary stays on topic and reads logically will reflect better code understanding. An LLM could be prompted to flag if a summary contains irrelevant information or contradictions.
	•	Length and Readability Metrics: Track the length of each summary (in tokens or sentences) and possibly readability (e.g., using a grade-level formula). You don’t want extremely long-winded summaries, nor ones too short to convey substance. If you find some models always produce overly long texts, you might include a penalty or lower conciseness score accordingly. Your conciseness metric (10%) already handles verbosity to an extent. You could make it more quantitative by defining an optimal length range for file vs symbol summaries (perhaps based on what users find readable in your UI) and scoring models higher when they fall in that range.
	•	Multi-Language Support Validation: Since you support ~15 languages via Tree-sitter, ensure your evaluation covers a representative set of languages. Different models might excel in certain languages (e.g., an open-source model fine-tuned on Python code vs. a general model might differ). You may want to stratify results by language – for example, ensure each model is tested on Python, JavaScript, Go, etc., and see if any model has blind spots. The AST-based checks might need slight adaptation per language (e.g., checking for exceptions thrown in Java vs. Python). If extending to more languages or frameworks, add any language-specific facts to ground truth extraction (for instance, in Java, note if a method is an override or implements an interface; in Go, note if a function is part of a struct’s methods, etc.).
	•	Maintainability of Evaluation Pipeline: As you iterate, keep an eye on how stable the rankings are. If small changes in prompt or test cases cause large swings, it might indicate the evaluation is not yet fully robust. One methodology is A/B testing with statistical significance – treat each pair of models and test cases, and see if one model significantly outperforms the other on the majority of cases (like a sign test or Wilcoxon test on the per-case scores). This prevents being misled by minor score differences. Research has shown that small differences in automated metric scores (a few BLEU points, etc.) often don’t mean real quality differences ￼. So focus on metrics that yield clear separation, and consider a margin or confidence interval when declaring one model “better” than another. This ties into your current practice of noting that <5% difference might not be significant – a good practice to continue.
	•	Human-in-the-Loop for Edge Cases: If possible, incorporate a final sanity check on the top contenders with human evaluation. For example, after narrowing down with automated metrics and LLM judges, have a developer review sample summaries from the top 2 models. This could surface qualitative differences that metrics don’t catch (maybe one model writes in a more explanatory style that developers prefer). Human feedback could then be used to adjust weighting of metrics (e.g., if humans consistently favor slightly longer, more explanatory summaries, you might increase weight on completeness/usefulness over conciseness).
	•	Cost and Speed as Secondary Metrics: While quality is the main goal, continue to record cost per summary and generation time. These should be reported separately so that stakeholders can consider them. For instance, you might present a Quality-Performance trade-off: Model X has the highest quality score but Model Y is nearly as good and 3× faster/cheaper. Depending on use-case, one might choose the slightly lower-quality model if it’s drastically cheaper for large codebases. So, it’s wise to keep these metrics visible (as you do in your reporting table) but perhaps not mix them into the core “quality” score too heavily unless production constraints demand it. One idea is to produce a Pareto chart or scatter plot (Quality vs Cost) to visualize which models are dominating or offering the best bang-for-buck.

Pipeline Improvement Suggestions

Integrating the above methodologies may require some enhancements to your 5-phase pipeline:
	•	Enhanced Test Case Selection: Ensure that your test cases (files/symbols) cover a variety of scenarios – different sizes, complexities, and domains. Include some “tricky” code files where understanding the intent is hard (to see if the models truly grok the code or just rephrase it). Also include some with existing comments to compare against (if available) as a form of ground truth for reference-based metrics.
	•	Advanced Judges (Consensus & Role-based): You already have support for multiple judges and blind evaluation. Consider adding the role-playing evaluation mode as an option (e.g., the consensus-judge could be extended to use different prompt templates or multiple passes as different roles). This might improve the quality scoring consistency even further. The research shows LLM evaluators can reach ~0.82 correlation with humans, outperforming simpler metrics by a large margin ￼ – aiming for that level of reliability in your judge phase is a good target.
	•	Composite Scoring Adjustments: Revisit the weighting of metrics once you incorporate new ones. For example, if you add an embedding similarity or IR-based metric, you might include it in the “Correctness” or “Quality” portion of the composite. You could pilot a few weight configurations and see which best aligns with human preference on a small set. It’s also worth deciding if Speed/Cost (the 20% performance metrics) should be included in the single “Overall” rank or shown as separate ranks. If pure quality is paramount, you might present both “Quality Score” and “Overall Score” (which includes performance). This way, one can identify the best-quality model and then consider if it’s worth any speed/cost trade-off separately.
	•	Continuous Updates and Learning: As new models (or new versions of Claude/GPT) come out, your evaluation system should be able to accommodate them easily. Keep an eye on emerging evaluation techniques too. For example, QAG (Question-Answer Generation) evaluation is a growing approach: an LLM generates questions based on the code or a reference summary, and then checks if the generated summary can answer those questions ￼ ￼. This tests the summary’s content in a targeted way. Implementing a Q&A-based evaluator could be a research area to consider for future improvements – essentially a dynamic way to check if knowledge from the code is present in the summary.

In summary, combining AST-driven factual checks, embedding-based semantic measures, LLM/human judgments, and search-oriented evaluations will give you a 360° view of model performance. This ensures the “best model” isn’t just the one with fancy language, but truly the one with better code understanding as evidenced by accuracy and utility. By integrating these methodologies into your pipeline, you can confidently compare LLMs (commercial and open-source alike) and choose the one that produces the most informative, correct, and helpful code summaries for your navigation tool.

References
	•	Automated vs. human metrics for code summary quality – n-gram metrics (BLEU, ROUGE) have low correlation with human judgments, whereas embedding-based and LLM evaluator approaches are more promising ￼ ￼.
	•	LLM-as-a-Judge technique – using large models to grade outputs with a rubric – is shown to be one of the most reliable modern evaluation methods ￼. Role-based evaluator prompts (e.g. CodeRPE) reached ~82% correlation with human ratings, far surpassing traditional metrics ￼.
	•	It’s recommended to combine multiple metrics: “Our method prefers outputs from the same LLM, so we recommend using it alongside an embedding-based metric.” ￼ – i.e., use semantic similarity checks to balance any single evaluator’s bias.
	•	For search relevance, leveraging embeddings for code–query similarity is effective: “To perform a code search, we embed the query in natural language… then calculate cosine similarity… The highest similarity results are most relevant.” ￼ This principle can be used to evaluate how well a summary could serve as a retrieval cue.
	•	Human evaluations historically focus on content adequacy (correctness/completeness) and conciseness ￼ – ensure your metrics align with these, as your current design does (50% automated factual, 30% quality via judges, 20% performance). Keeping metrics grounded in human relevance is key to judging “better code understanding.”
