/**
 * Benchmark V2 Types
 *
 * Complete type definitions for the LLM code summary evaluation benchmark.
 * Implements the 4-phase evaluation pipeline:
 *   Phase 1: Extraction (code units from codebase)
 *   Phase 2: Generation (summaries from each model)
 *   Phase 3: Evaluation (Judge, Contrastive, Retrieval, Downstream)
 *   Phase 4: Aggregation & Reporting
 */

import type { CodeChunk, LLMProvider } from "../types.js";

// ============================================================================
// Code Unit Types (Phase 1: Extraction)
// ============================================================================

/** Types of code units that can be benchmarked */
export type CodeUnitType = "function" | "class" | "method" | "file" | "module";

/** Parameter definition from AST */
export interface Parameter {
	name: string;
	type?: string;
	description?: string;
	optional: boolean;
	defaultValue?: string;
}

/** AST-derived metadata for a code unit */
export interface CodeUnitMetadata {
	startLine: number;
	endLine: number;
	signature?: string;
	parameters?: Parameter[];
	returnType?: string;
	visibility?: "public" | "private" | "protected";
	decorators?: string[];
	dependencies: string[];
	exports?: string[];
	complexity?: number;
	isAsync?: boolean;
}

/** Relationships extracted from AST */
export interface CodeUnitRelationships {
	parentId?: string;
	childIds: string[];
	callsIds: string[];
	calledByIds: string[];
}

/**
 * A code unit represents a single extractable piece of code
 * (function, class, method, or file) that will be summarized.
 *
 * Note: We extend CodeChunk from the main codebase where applicable,
 * but add benchmark-specific fields for full spec compliance.
 */
export interface BenchmarkCodeUnit {
	/** Unique identifier (hash of content + path) */
	id: string;
	/** File path relative to repo root */
	path: string;
	/** Function/class/file name */
	name: string;
	/** Type of code unit */
	type: CodeUnitType;
	/** Programming language */
	language: string;
	/** The actual code content */
	content: string;
	/** AST-derived metadata */
	metadata: CodeUnitMetadata;
	/** Relationships from AST */
	relationships: CodeUnitRelationships;
	/** Original CodeChunk reference (for reusing existing infrastructure) */
	sourceChunk?: CodeChunk;
}

// ============================================================================
// Generated Summary Types (Phase 2: Generation)
// ============================================================================

/** Metadata about how a summary was generated */
export interface GenerationMetadata {
	modelName: string;
	modelVersion: string;
	promptVersion: string;
	temperature: number;
	maxTokens: number;
	generatedAt: string;
	latencyMs: number;
	inputTokens: number;
	outputTokens: number;
	cost?: number;
}

/** A summary generated by an LLM for a code unit */
export interface GeneratedSummary {
	/** Unique identifier */
	id: string;
	/** Reference to the code unit */
	codeUnitId: string;
	/** Which model generated this */
	modelId: string;
	/** The generated summary text */
	summary: string;
	/** Generation metadata */
	generationMetadata: GenerationMetadata;
}

// ============================================================================
// Evaluation Result Types (Phase 3: Evaluation)
// ============================================================================

/** Types of evaluations supported */
export type EvaluationType = "judge" | "contrastive" | "retrieval" | "downstream" | "self" | "iterative";

/** Scores from judge evaluation (1-5 scale) */
export interface JudgeScores {
	accuracy: number;
	completeness: number;
	semanticRichness: number;
	abstraction: number;
	conciseness: number;
}

/** Results from LLM-as-Judge evaluation */
export interface JudgeResults {
	judgeModelId: string;
	scores: JudgeScores;
	reasoning: string;
	weightedAverage: number;
	pairwiseWins?: number;
	pairwiseLosses?: number;
	pairwiseTies?: number;
	/** Cost of this judge evaluation in USD */
	cost?: number;
}

/** Method used for contrastive matching */
export type ContrastiveMethod = "embedding" | "llm";

/** Results from contrastive matching evaluation */
export interface ContrastiveResults {
	correct: boolean;
	predictedRank: number;
	distractorIds: string[];
	method: ContrastiveMethod;
	confidenceGap?: number;
	embeddingModel?: string;
	llmModel?: string;
}

/** Results from retrieval evaluation */
export interface RetrievalResults {
	queryId: string;
	queryType: string;
	query: string;
	hitAtK: Record<number, boolean>;
	reciprocalRank: number;
	retrievedRank: number | null;
	// Cross-model competition fields (optional for backward compatibility)
	/** Rank among models (1 = this model's summary ranked highest) */
	modelRank?: number;
	/** Total number of models competing */
	totalModels?: number;
	/** Did this model win (rank #1 among models)? */
	isWinner?: boolean;
	/** Total items in the combined index */
	poolSize?: number;
}

/** Downstream task types */
export type DownstreamTaskType = "completion" | "bug_localization" | "function_selection";

/** Results from downstream task evaluation */
export interface DownstreamResults {
	taskType: DownstreamTaskType;
	taskId: string;
	success: boolean;
	partialScore?: number;
	details?: Record<string, unknown>;
}

/** Self-evaluation task types */
export type SelfEvalTaskType = "retrieval" | "completion" | "function_selection";

/** Results from self-evaluation (model uses its own summaries) */
export interface SelfEvaluationResults {
	/** The generating model that was tested */
	generatingModelId: string;
	/** Type of self-evaluation task */
	taskType: SelfEvalTaskType;
	/** For retrieval: did the model find the right code using its own summary? */
	retrievalResults?: {
		queryId: string;
		query: string;
		/** Did the model correctly identify the source code from its summary? */
		correct: boolean;
		/** Confidence score (0-1) from the model */
		confidence: number;
		/** Model's reasoning for its choice */
		reasoning?: string;
	};
	/** For completion: could the model complete code using its own summary? */
	completionResults?: {
		taskId: string;
		/** BLEU score of completion */
		bleuScore: number;
		/** Did it pass tests? */
		passedTests: boolean;
		/** Model's generated completion */
		completion: string;
	};
	/** For function selection: could the model pick the right function using its summary? */
	functionSelectionResults?: {
		taskId: string;
		/** Did it select the correct function? */
		correct: boolean;
		/** Which function was selected */
		selectedFunction: string;
		/** Model's reasoning */
		reasoning?: string;
	};
}

/** Results from iterative refinement evaluation */
export interface IterativeResults {
	/** The model that generated the summary */
	modelId: string;
	/** Code unit this summary is for */
	codeUnitId: string;
	/** Number of refinement rounds executed (0 = initial was good) */
	rounds: number;
	/** Whether target rank was achieved */
	success: boolean;
	/** Initial summary quality rank */
	initialRank: number | null;
	/** Final summary quality rank */
	finalRank: number | null;
	/** Brokk-style score: 1.0 / log2(rounds + 2) */
	refinementScore: number;
	/** History of all refinement attempts */
	history: Array<{
		round: number;
		rank: number | null;
		passed: boolean;
		summary?: string;
	}>;
	/** Strategy used for quality testing */
	strategyName: string;
	/** The final refined summary (if different from original) */
	refinedSummary?: string;
	/** Total time spent on refinement */
	durationMs: number;
}

/** Complete evaluation result for a summary */
export interface EvaluationResult {
	id: string;
	summaryId: string;
	evaluationType: EvaluationType;
	judgeResults?: JudgeResults;
	contrastiveResults?: ContrastiveResults;
	retrievalResults?: RetrievalResults;
	downstreamResults?: DownstreamResults;
	selfEvaluationResults?: SelfEvaluationResults;
	iterativeResults?: IterativeResults;
	evaluatedAt: string;
}

// ============================================================================
// Pairwise Comparison Types (for Judge Evaluation)
// ============================================================================

/** Result of a pairwise comparison between two summaries */
export interface PairwiseResult {
	modelA: string;
	modelB: string;
	codeUnitId: string;
	judgeModel: string;
	winner: "A" | "B" | "tie";
	confidence: "high" | "medium" | "low";
	positionSwapped: boolean;
	reasoning?: string;
	criteriaBreakdown?: {
		accuracy: "A" | "B" | "tie";
		completeness: "A" | "B" | "tie";
		searchability: "A" | "B" | "tie";
		clarity: "A" | "B" | "tie";
		conciseness: "A" | "B" | "tie";
	};
	/** Cost of this comparison in USD (may be portion of batched call) */
	cost?: number;
}

/** Tournament scores for a model */
export interface TournamentScore {
	wins: number;
	losses: number;
	ties: number;
	winRate: number;
	btScore: number; // Bradley-Terry score
}

// ============================================================================
// Query Types (for Retrieval Evaluation)
// ============================================================================

/** Types of search queries for retrieval testing */
export type QueryType =
	| "vague"
	| "wrong_terminology"
	| "specific_behavior"
	| "integration"
	| "problem_based";

/** A generated search query for testing retrieval */
export interface GeneratedQuery {
	id: string;
	codeUnitId: string;
	type: QueryType;
	query: string;
	shouldFind: boolean;
}

// ============================================================================
// Downstream Task Types
// ============================================================================

/** Code completion task */
export interface CompletionTask {
	id: string;
	codeUnitId: string;
	partialCode: string;
	fullCode: string;
	requirements: string;
	language: string;
	relevantSummaryIds: string[];
	testCases?: Array<{
		input: string;
		expectedOutput: string;
	}>;
}

/** Bug localization task */
export interface BugLocalizationTask {
	id: string;
	bugDescription: string;
	actualBuggyFile: string;
	candidateFiles: string[];
}

/** Function selection task */
export interface FunctionSelectionTask {
	id: string;
	taskDescription: string;
	correctFunction: string;
	candidateFunctions: string[];
}

// ============================================================================
// Distractor Types (for Contrastive Evaluation)
// ============================================================================

/** Difficulty level for distractor sets */
export type DistractorDifficulty = "easy" | "medium" | "hard";

/** Set of distractors for a target code unit */
export interface DistractorSet {
	targetCodeUnitId: string;
	distractorIds: string[];
	difficulty: DistractorDifficulty;
}

// ============================================================================
// Scoring Types (Phase 4: Aggregation)
// ============================================================================

/** Default weights for combining judge scores */
export const JUDGE_SCORE_WEIGHTS = {
	accuracy: 0.25,
	completeness: 0.20,
	semanticRichness: 0.25,
	abstraction: 0.15,
	conciseness: 0.15,
} as const;

/**
 * Weights for combining quality evaluation metrics.
 *
 * These metrics measure how well summaries serve LLM agents:
 * - Retrieval: Can agents FIND the right code? (semantic search)
 * - Contrastive: Can agents DISTINGUISH similar code?
 * - Judge: Is the summary accurate and complete?
 *
 * Operational metrics (latency, cost, refinement, self-eval) are
 * reported separately and don't affect the quality score.
 */
export interface EvaluationWeights {
	/** Retrieval quality (P@K, MRR) - most critical for code search */
	retrieval: number;
	/** Contrastive accuracy - distinguishes code among distractors */
	contrastive: number;
	/** Judge score - accuracy, completeness, quality */
	judge: number;
	/** @deprecated Use operational metrics instead */
	downstream?: number;
	/** @deprecated Use operational metrics instead */
	iterative?: number;
}

/**
 * Default evaluation weights optimized for LLM agent code understanding.
 *
 * Rationale:
 * - Retrieval (45%): If agents can't find code, nothing else matters
 * - Contrastive (30%): Agents must distinguish similar functions
 * - Judge (25%): Quality baseline for accuracy/completeness
 */
export const DEFAULT_EVALUATION_WEIGHTS: EvaluationWeights = {
	retrieval: 0.45,
	contrastive: 0.30,
	judge: 0.25,
};

/** Weights for combining judge evaluation methods */
export interface JudgeWeights {
	pointwise: number;
	pairwise: number;
}

/** Weights for combining contrastive methods */
export interface ContrastiveWeights {
	embedding: number;
	llm: number;
}

/** Weights for combining retrieval metrics */
export interface RetrievalWeights {
	precision1: number;
	precision5: number;
	mrr: number;
}

/** Weights for combining downstream tasks */
export interface DownstreamWeights {
	completion: number;
	bugLocalization: number;
	functionSelection: number;
}

/** Complete scoring configuration */
export interface ScoringConfig {
	judgeWeights: JudgeWeights;
	contrastiveWeights: ContrastiveWeights;
	retrievalWeights: RetrievalWeights;
	downstreamWeights: DownstreamWeights;
	evalWeights: EvaluationWeights;
}

/** Default scoring configuration */
export const DEFAULT_SCORING_CONFIG: ScoringConfig = {
	judgeWeights: {
		pointwise: 0.4,
		pairwise: 0.6,
	},
	contrastiveWeights: {
		embedding: 0.5,
		llm: 0.5,
	},
	retrievalWeights: {
		precision1: 0.3,
		precision5: 0.4,
		mrr: 0.3,
	},
	downstreamWeights: {
		completion: 0.4,
		bugLocalization: 0.3,
		functionSelection: 0.3,
	},
	evalWeights: DEFAULT_EVALUATION_WEIGHTS,
};

/** Normalized scores for a model (all 0-1 scale) */
export interface NormalizedScores {
	modelId: string;
	judge: {
		pointwise: number;
		pairwise: number;
		combined: number;
	};
	contrastive: {
		embedding: number;
		llm: number;
		combined: number;
	};
	retrieval: {
		precision1: number;
		precision5: number;
		mrr: number;
		winRate?: number;
		combined: number;
	};
	downstream: {
		completion: number;
		bugLocalization: number;
		functionSelection: number;
		combined: number;
	};
	overall: number;
	/** Operational: Iterative refinement metrics (optional) */
	iterative?: {
		avgRounds: number;
		successRate: number;
		avgRefinementScore: number;
	};
	/** Operational: Self-evaluation metrics (optional) */
	self?: {
		overall: number;
		retrieval: number;
		functionSelection: number;
	};
}

// ============================================================================
// Model Configuration Types
// ============================================================================

/** Supported model providers */
export type ModelProvider = "anthropic" | "openai" | "google" | "openrouter" | "local";

/** Configuration for a model under test */
export interface ModelConfig {
	id: string;
	provider: ModelProvider;
	modelName: string;
	displayName?: string;
	apiEndpoint?: string;
	temperature: number;
	maxTokens: number;
}

// ============================================================================
// Benchmark Configuration Types
// ============================================================================

/** Sampling strategies for selecting code units */
export type SamplingStrategy = "random" | "stratified" | "all";

/** Evaluation configuration */
export interface EvaluationConfig {
	judge: {
		enabled: boolean;
		judgeModels: string[];
		usePairwise: boolean;
	};
	contrastive: {
		enabled: boolean;
		distractorCount: number;
		method: "embedding" | "llm" | "both";
		embeddingModel?: string;
	};
	retrieval: {
		enabled: boolean;
		queriesPerUnit: number;
		kValues: number[];
		embeddingModel?: string;
	};
	downstream: {
		enabled: boolean;
		tasks: DownstreamTaskType[];
		completionModel?: string;
	};
	/** Self-evaluation: generating model tests its own summaries */
	self: {
		enabled: boolean;
		/** Tasks to run: retrieval (can model find code from its summary?), completion, function_selection */
		tasks: SelfEvalTaskType[];
		/** Number of retrieval queries per code unit */
		queriesPerUnit: number;
	};
	/** Iterative refinement: refine summaries until they rank well */
	iterative: {
		enabled: boolean;
		/** Maximum refinement rounds per summary (default: 3) */
		maxRounds: number;
		/** Target rank for success (e.g., 3 = top-3) */
		targetRank: number;
		/** Strategy for quality testing */
		strategy: "retrieval" | "bleu" | "llm-judge";
		/** Apply Brokk-style scoring penalty based on rounds */
		applyRoundsPenalty: boolean;
		/** Max items to refine per model (default: 10, refinement is expensive) */
		sampleSize: number;
	};
}

/** Complete benchmark configuration */
export interface BenchmarkConfig {
	/** Name of this benchmark run */
	name: string;
	/** Description of what's being tested */
	description?: string;
	/** Path to the project to benchmark */
	projectPath: string;
	/** Generator models to test */
	generators: ModelConfig[];
	/** Judge models (user-selected) */
	judges: string[];
	/** Number of code units to test */
	sampleSize: number;
	/** How to sample code units */
	samplingStrategy: SamplingStrategy;
	/** Types of code units to include */
	codeUnitTypes: CodeUnitType[];
	/** Languages to include */
	languages?: string[];
	/** Evaluation configuration */
	evaluation: EvaluationConfig;
	/** Scoring weights */
	weights: ScoringConfig;
	/** Output formats to generate */
	outputFormats: ReportFormat[];
	/** Enable verbose logging */
	verbose?: boolean;
	/**
	 * Local model parallelism (lmstudio, ollama).
	 * - 0 = all in parallel (may cause model swapping if VRAM limited)
	 * - 1 = sequential (default, safest for limited VRAM)
	 * - 2-4 = run N local models concurrently
	 */
	localModelParallelism?: number;
}

// ============================================================================
// Benchmark Run Types
// ============================================================================

/** Status of a benchmark run */
export type BenchmarkStatus = "pending" | "running" | "completed" | "failed" | "paused";

/** Phase of the benchmark pipeline */
export type BenchmarkPhase =
	| "extraction"
	| "generation"
	| "evaluation:iterative"
	| "evaluation:judge"
	| "evaluation:contrastive"
	| "evaluation:retrieval"
	| "evaluation:downstream"
	| "evaluation:self"
	| "aggregation"
	| "reporting";

/** Information about the codebase being benchmarked */
export interface CodebaseInfo {
	name: string;
	repository?: string;
	commit?: string;
	languages: string[];
	totalCodeUnits: number;
	sampledCodeUnits: number;
}

/** A complete benchmark run */
export interface BenchmarkRun {
	id: string;
	name: string;
	description?: string;
	config: BenchmarkConfig;
	codebaseInfo: CodebaseInfo;
	modelsUnderTest: ModelConfig[];
	judgeModels: ModelConfig[];
	status: BenchmarkStatus;
	currentPhase?: BenchmarkPhase;
	startedAt: string;
	completedAt?: string;
	pausedAt?: string;
	error?: string;
}

// ============================================================================
// Reporting Types (Phase 4: Reporting)
// ============================================================================

/** Output format for reports */
export type ReportFormat = "json" | "markdown" | "html";

/** Model ranking entry */
export interface ModelRanking {
	rank: number;
	modelId: string;
	modelName: string;
	overallScore: number;
	scores: {
		judge: number;
		contrastive: number;
		retrieval: number;
		downstream: number;
	};
	deltaFromBaseline?: number;
}

/** Head-to-head model comparison */
export interface ModelComparison {
	modelA: string;
	modelB: string;
	winner: string;
	scoreDifference: number;
	significant: boolean;
	pValue?: number;
	pairwiseRecord: {
		aWins: number;
		bWins: number;
		ties: number;
	};
	strengthsA: string[];
	strengthsB: string[];
}

/** Failure analysis for a model */
export interface FailureAnalysis {
	modelId: string;
	weakestMetric: string;
	weakestLanguage: string;
	weakestCodeType: string;
	examples: Array<{
		codeUnitId: string;
		summary: string;
		issue: string;
		category: string;
	}>;
}

/** Cost breakdown by model */
export interface CostBreakdown {
	modelId: string;
	totalCost: number;
	costPerThousandSummaries: number;
	inputTokens: number;
	outputTokens: number;
}

/** Statistical significance test result */
export interface SignificanceTest {
	modelA: string;
	modelB: string;
	metric: string;
	meanDifference: number;
	pValue: number;
	significant: boolean;
	confidenceInterval: [number, number];
}

/** Complete benchmark report */
export interface BenchmarkReport {
	metadata: {
		benchmarkId: string;
		name: string;
		runDate: string;
		duration: string;
		codebase: CodebaseInfo;
		configuration: BenchmarkConfig;
	};
	rankings: ModelRanking[];
	detailed: {
		byModel: Map<string, NormalizedScores>;
		byLanguage: Map<string, Map<string, number>>;
		byCodeType: Map<string, Map<string, number>>;
	};
	comparisons: ModelComparison[];
	statistics: {
		significanceTests: SignificanceTest[];
	};
	failures: {
		byModel: Map<string, FailureAnalysis>;
		commonPatterns: Array<{
			pattern: string;
			frequency: number;
			description: string;
		}>;
	};
	costs: {
		byModel: Map<string, CostBreakdown>;
		total: number;
	};
}

// ============================================================================
// Progress Callback Types
// ============================================================================

/** Progress callback for benchmark operations */
export type BenchmarkProgressCallback = (
	phase: BenchmarkPhase,
	completed: number,
	total: number,
	details?: string
) => void;

// ============================================================================
// Interface Definitions (for implementers)
// ============================================================================

/** Summary generator interface */
export interface ISummaryGenerator {
	/** Generate a summary for a code unit */
	generateSummary(
		codeUnit: BenchmarkCodeUnit,
		promptVersion: string
	): Promise<GeneratedSummary>;
	/** Get model info */
	getModelInfo(): ModelConfig;
	/** Get accumulated usage stats */
	getUsageStats(): {
		inputTokens: number;
		outputTokens: number;
		cost: number;
		calls: number;
	};
	/** Reset usage tracking */
	resetUsage(): void;
}

/** Evaluator interface (for each evaluation type) */
export interface IEvaluator<TResult> {
	/** Run evaluation on a summary */
	evaluate(
		summary: GeneratedSummary,
		codeUnit: BenchmarkCodeUnit,
		context: EvaluatorContext
	): Promise<TResult>;
	/** Get evaluation type */
	getType(): EvaluationType;
}

/** Context passed to evaluators */
export interface EvaluatorContext {
	/** All code units (for contrastive/retrieval) */
	allCodeUnits?: BenchmarkCodeUnit[];
	/** All summaries by model (for comparisons) */
	allSummaries?: Map<string, GeneratedSummary[]>;
	/** Generated queries (for retrieval) */
	queries?: GeneratedQuery[];
	/** Distractor sets (for contrastive) */
	distractors?: DistractorSet[];
	/** Downstream tasks */
	tasks?: {
		completion?: CompletionTask[];
		bugLocalization?: BugLocalizationTask[];
		functionSelection?: FunctionSelectionTask[];
	};
}

/** Reporter interface */
export interface IReporter {
	/** Generate report from benchmark results */
	generate(report: BenchmarkReport): Promise<string>;
	/** Get the format this reporter produces */
	getFormat(): ReportFormat;
}

// ============================================================================
// Database Schema Types (for SQLite persistence)
// ============================================================================

/** Database row types for SQLite persistence */
export interface DBBenchmarkRun {
	id: string;
	name: string;
	description: string | null;
	config_json: string;
	codebase_info_json: string;
	status: BenchmarkStatus;
	current_phase: BenchmarkPhase | null;
	started_at: string;
	completed_at: string | null;
	paused_at: string | null;
	error: string | null;
}

export interface DBCodeUnit {
	id: string;
	run_id: string;
	path: string;
	name: string;
	type: CodeUnitType;
	language: string;
	content: string;
	metadata_json: string;
	relationships_json: string;
}

export interface DBGeneratedSummary {
	id: string;
	run_id: string;
	code_unit_id: string;
	model_id: string;
	summary: string;
	generation_metadata_json: string;
}

export interface DBEvaluationResult {
	id: string;
	run_id: string;
	summary_id: string;
	evaluation_type: EvaluationType;
	results_json: string;
	evaluated_at: string;
}

export interface DBPairwiseResult {
	id: string;
	run_id: string;
	model_a: string;
	model_b: string;
	code_unit_id: string;
	judge_model: string;
	winner: "A" | "B" | "tie";
	confidence: string;
	position_swapped: boolean;
	reasoning: string | null;
	criteria_breakdown_json: string | null;
	cost: number | null;
}

export interface DBGeneratedQuery {
	id: string;
	run_id: string;
	code_unit_id: string;
	type: QueryType;
	query: string;
	should_find: boolean;
}

// ============================================================================
// Additional Types for Scorers and Reporters
// ============================================================================

/** Aggregated score for a model (used in reports) */
export interface AggregatedScore {
	modelId: string;
	judgeScore: number;
	contrastiveAccuracy: number;
	retrievalMRR: number;
	retrievalPrecision: Record<number, number>;
	downstreamScore: number;
	overallScore: number;
	rank: number;
}

/** Model-level score summary */
export interface ModelScore {
	modelId: string;
	scores: NormalizedScores;
	rank: number;
}

// ============================================================================
// Extended Config Types (used by index.ts)
// ============================================================================

/** Sampling configuration */
export interface SamplingConfig {
	strategy: SamplingStrategy;
	targetCount: number;
	maxPerFile?: number;
	minComplexity?: number;
}

/** Judge evaluation configuration */
export interface JudgeEvaluationConfig {
	enabled: boolean;
	judgeModels: string[];
	usePairwise: boolean;
	criteriaWeights?: {
		accuracy: number;
		completeness: number;
		semanticRichness: number;
		abstraction: number;
		conciseness: number;
	};
}

/** Contrastive evaluation configuration */
export interface ContrastiveEvaluationConfig {
	enabled: boolean;
	method: "embedding" | "llm" | "both";
	distractorCount: number;
	embeddingModel?: string;
}

/** Retrieval evaluation configuration */
export interface RetrievalEvaluationConfig {
	enabled: boolean;
	kValues: number[];
	queryTypes?: QueryType[];
}

/** Downstream evaluation configuration */
export interface DownstreamEvaluationConfig {
	enabled: boolean;
	tasks: {
		codeCompletion: boolean;
		bugLocalization: boolean;
		functionSelection: boolean;
	};
}

/** Extended scoring config with weights */
export interface ExtendedScoringConfig {
	weights: EvaluationWeights;
	normalization: "min-max" | "z-score" | "percentile";
}
